{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-07T20:20:09.253084Z",
     "start_time": "2019-12-07T20:20:09.235773Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "merge.py\n",
    "reads in NOAA converted coordinates and merge with accident, road, grade, curve files\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:14:05.039483Z",
     "start_time": "2019-12-08T16:14:04.473180Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read noaa coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:14:05.164178Z",
     "start_time": "2019-12-08T16:14:05.156548Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_noaa_coords(yr, directory):\n",
    "    columns = ['ID', 'destLat', 'destLon']\n",
    "    \n",
    "    # detect all csv files of the year\n",
    "    yr_file_list = []\n",
    "    for file in os.listdir(directory):\n",
    "        if str(yr) in file:\n",
    "            yr_file_list.append(file)\n",
    "    \n",
    "    # sort so that No.0 file is at the first place\n",
    "    yr_file_list = sorted(yr_file_list)\n",
    "    \n",
    "    # read and append the dataframes\n",
    "    count = 0\n",
    "    for file in yr_file_list:\n",
    "        if count == 0:\n",
    "            records = pd.read_csv(directory + '/' + file)\n",
    "            records = records[columns]\n",
    "        else:\n",
    "            records = records.append(pd.read_csv(directory + '/' + file)[columns]).reset_index(drop=True)\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    records.columns = ['ID', 'lat', 'lon']\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### detect files with keywords in name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:14:06.946338Z",
     "start_time": "2019-12-08T16:14:06.933640Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_files(directory, keyword):\n",
    "    file_list = []\n",
    "    for file in os.listdir(directory):\n",
    "        if keyword in file:\n",
    "            file_list.append(file)\n",
    "    \n",
    "    return sorted(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge noaa coords with acc and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:14:07.529138Z",
     "start_time": "2019-12-08T16:14:07.519463Z"
    }
   },
   "outputs": [],
   "source": [
    "def acc_merge(acc_file_list, noaa_coords, directory):\n",
    "    crashes = {}\n",
    "    for file in acc_file_list:\n",
    "        yr = file[2:4]\n",
    "        acc_file = directory + '/' + file\n",
    "        tmp = pd.read_csv(acc_file)\n",
    "        tmp = tmp.dropna(subset=['State_Plane_X', 'State_Plane_Y'])\n",
    "        tmp['ID'] = tmp.index + 1\n",
    "        crashes[2000+int(yr)] = tmp\n",
    "    \n",
    "    for yr in noaa_coords.keys():\n",
    "        crashes[yr] = crashes[yr].merge(noaa_coords[yr], on='ID', how='inner')\n",
    "    \n",
    "    columns = ['CASENO', 'FORM_REPT_NO', 'rd_inv', 'milepost', 'RTE_NBR',\n",
    "           'lat', 'lon', \n",
    "           'MONTH', 'DAYMTH', 'WEEKDAY', \n",
    "           'RDSURF', 'LIGHT', 'weather', 'rur_urb',\n",
    "           'REPORT', 'SEVERITY']\n",
    "\n",
    "    for yr in crashes.keys():\n",
    "        crashes[yr] = crashes[yr][columns]\n",
    "    \n",
    "    return crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read files with keyword and directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:14:09.475246Z",
     "start_time": "2019-12-08T16:14:09.465938Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_files(directory, keyword):\n",
    "    output_dic = {}\n",
    "    file_list = detect_files(directory, keyword)\n",
    "    for yr in range(2013, 2018):\n",
    "        output_dic[yr] = pd.read_csv(directory + '/' + file_list[yr-2013])\n",
    "    return output_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### final meta merge func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:14:09.956572Z",
     "start_time": "2019-12-08T16:14:09.938150Z"
    }
   },
   "outputs": [],
   "source": [
    "def meta_merge(crashes, curv, grad, occ, road, veh):\n",
    "    \"\"\"\n",
    "    they have the same keys (of year 2013 to 2017)\n",
    "    \"\"\"\n",
    "    meta = {}\n",
    "    for yr in range(2013, 2018):\n",
    "        # first merge veh count with crashes\n",
    "        veh_count = veh[yr]['CASENO'].value_counts().sort_index()\n",
    "        veh_count = veh_count.to_frame().reset_index()\n",
    "        veh_count.columns = ['CASENO', 'veh_count']\n",
    "        \n",
    "        acc_this_yr = crashes[yr].merge(veh_count, on='CASENO', how='inner')\n",
    "        \n",
    "        # road\n",
    "        road_this_yr = road[yr].drop(['RTE_NBR'], axis=1)\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        acc_this_yr.to_sql(\"crash\", conn, index=False)\n",
    "        road_this_yr.to_sql(\"road\", conn, index=False)\n",
    "        query = \\\n",
    "            \"SELECT * \\\n",
    "            FROM crash, road\\\n",
    "            WHERE crash.rd_inv = road.ROAD_INV\\\n",
    "            AND crash.milepost >= road.BEGMP\\\n",
    "            AND crash.milepost <= road.ENDMP\"\n",
    "        records = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        ## remove duplicates randomly\n",
    "        records = records.sample(frac=1).drop_duplicates(subset='CASENO').sort_index()\n",
    "        \n",
    "        # curve\n",
    "        curv_this_yr = curv[yr]\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        records.to_sql(\"records\", conn, index=False)\n",
    "        curv_this_yr.to_sql(\"curv\", conn, index=False)\n",
    "        \n",
    "        query = \\\n",
    "            \"SELECT * \\\n",
    "            FROM records LEFT JOIN curv ON records.rd_inv = curv.curv_inv\\\n",
    "            AND records.milepost >= curv.begmp\\\n",
    "            AND records.milepost <= curv.endmp\"\n",
    "        records = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        ## remove duplicates and drop useless attributes\n",
    "        records = records.sample(frac=1).drop_duplicates(subset='CASENO').sort_index()\n",
    "        records = records.drop(['curv_inv', 'begmp', 'endmp', 'rte_nbr', 'DIR_CURV'], axis=1)\n",
    "        \n",
    "        ## fill NaN curvature with 0\n",
    "        records = records.fillna(value={'deg_curv': 0})\n",
    "        \n",
    "        # grad\n",
    "        grad_this_yr = grad[yr]\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        records.to_sql(\"records\", conn, index=False)\n",
    "        grad_this_yr.to_sql(\"grad\", conn, index=False)\n",
    "        \n",
    "        query = \\\n",
    "            \"SELECT * \\\n",
    "            FROM records LEFT JOIN grad ON records.rd_inv = grad.grad_inv\\\n",
    "            AND records.milepost >= grad.begmp\\\n",
    "            AND records.milepost <= grad.endmp\"\n",
    "        \n",
    "        records = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        records = records.drop(['grad_inv', 'begmp', 'endmp', 'rte_nbr'], axis=1)\n",
    "        records = records.sample(frac=1).drop_duplicates(subset='CASENO').sort_index()\n",
    "        \n",
    "        records = records.fillna(value={'pct_grad': 0})\n",
    "    \n",
    "        # when everything is done, save\n",
    "        meta[yr] = records\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read NOAA converted coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:14:15.016729Z",
     "start_time": "2019-12-08T16:14:11.739769Z"
    }
   },
   "outputs": [],
   "source": [
    "noaa_coords = {}\n",
    "for yr in range(2013, 2018):\n",
    "    noaa_coords[yr] = read_noaa_coords(yr, '../../data/coords-noaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge NOAA coodinates to acc records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### detect and read all acc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:14:16.463132Z",
     "start_time": "2019-12-08T16:14:15.019393Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_file_list = detect_files(\"../../data/hsis-csv\", 'acc')\n",
    "crashes = acc_merge(acc_file_list, noaa_coords, '../../data/hsis-csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with other files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read them first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:14:19.175840Z",
     "start_time": "2019-12-08T16:14:17.836138Z"
    }
   },
   "outputs": [],
   "source": [
    "curv = read_files(\"../../data/hsis-csv\", 'curv')\n",
    "grad = read_files(\"../../data/hsis-csv\", 'grad')\n",
    "occ = read_files(\"../../data/hsis-csv\", 'occ')\n",
    "road = read_files(\"../../data/hsis-csv\", 'road')\n",
    "veh = read_files(\"../../data/hsis-csv\", 'veh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### meta merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T16:17:48.343310Z",
     "start_time": "2019-12-08T16:14:20.130224Z"
    }
   },
   "outputs": [],
   "source": [
    "met = meta_merge(crashes, curv, grad, occ, road, veh)\n",
    "for yr in range(2013, 2018):\n",
    "    met[yr].to_csv('../../data/crash-merged/{}.csv'.format(yr), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
